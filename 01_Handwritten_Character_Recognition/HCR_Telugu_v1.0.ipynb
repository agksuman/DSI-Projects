{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary packages required to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:15.261098Z",
     "start_time": "2020-03-24T23:33:11.812447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the summary of the steps preformed in the function clean_image\n",
    "\n",
    "* Loaded image pixel data into a NumPy array using Image package\n",
    "- To remove the noise of the different colors (color of page etc.) the pixel where the R-G-B values are less than 100 are made pure white pixels with value (255,255,255) \n",
    "- To increase the character strength the Blue color intensity is improved where the pixels are identified with color of the pen written (In the current scenario the color of the text is blue). This module would require slight modification with color of the text is different from blue. \n",
    "- Convert the image to monochrome and then to Black & White.\n",
    "- The image of then flipped to black to white and vise versa\n",
    "- To improve the quality of the image removed the Pixels for which there is only one neighbouring pixel. One of technique used to remove the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:15.277738Z",
     "start_time": "2020-03-24T23:33:15.262114Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_image(img,skip_lines=10):\n",
    "    \n",
    "    ## Get the pixel map of the image\n",
    "    pixelMap = img.load()\n",
    "    \n",
    "    ## Increase the contrast of the image by manipulating the RGB values\n",
    "    for i in range(img.size[0]):\n",
    "        for j in range(img.size[1]):\n",
    "                if pixelMap[i,j][0] < 100 and pixelMap[i,j][1] < 100 and pixelMap[i,j][2] < 100 :\n",
    "                    pixelMap[i,j] = (255, 255, 255)\n",
    "                elif pixelMap[i,j][0] < 200 and pixelMap[i,j][1] < 200:\n",
    "                    pixelMap[i,j] = (0, 0, 255)\n",
    "    \n",
    "    # convert image to monochrome\n",
    "    img_mc = img.convert('L')\n",
    "    # convert image to black and white\n",
    "    img_bw = img_mc.convert('1')\n",
    "    \n",
    "    ## Getting pixel map for Black & White Image\n",
    "    pixelMap_bw = img_bw.load()\n",
    "    \n",
    "    ## Flipping the colours black and white\n",
    "    for i in range(img_bw.size[0]):\n",
    "        for j in range(img_bw.size[1]):\n",
    "            pixelMap_bw[i,j] = 255 - pixelMap_bw[i,j]\n",
    "    \n",
    "    ## Further to enhance the image, removing all the white dots of the image\n",
    "    for i in range(img_bw.size[0]):\n",
    "        for j in range(img_bw.size[1]):\n",
    "            if i < skip_lines or j < skip_lines or i >= img_bw.size[0]-skip_lines or j >= img_bw.size[1]-skip_lines:\n",
    "                pixelMap_bw[i,j] = 0\n",
    "                continue\n",
    "            s = pixelMap_bw[i-1,j-1] + pixelMap_bw[i,j-1] + pixelMap_bw[i+1,j-1]\n",
    "            s = s + pixelMap_bw[i-1,j] + pixelMap_bw[i+1,j]\n",
    "            s = s + pixelMap_bw[i-1,j+1] + pixelMap_bw[i,j+1] + pixelMap_bw[i+1,j+1]\n",
    "            if s < 256:\n",
    "                pixelMap_bw[i,j] = 0\n",
    "    \n",
    "    ## Return the result image\n",
    "    return img_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding_clusters function uses DBScan to find clusters of pixels in image\n",
    "- Converting the image-array into data points in 2 dimensional canvas by taking the indices of the white color pixels of images as x,y co-ordinates.\n",
    "- All the two-dimensional data points are processed through DBScan to identify the clusters\n",
    "- For DBScan, eps is set to 2 and minimum samples to 3\n",
    "- The x,y co-ordinates and the labels for each cluster are returned in data frame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:16.112285Z",
     "start_time": "2020-03-24T23:33:16.095629Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_clusters(img,eps=2,min_samples=3):\n",
    "    \n",
    "    ## Get the image file into an array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    ## Get the data points in a dataframe to use DBScan to identify clusters\n",
    "    x = []\n",
    "    y = []\n",
    "    for i,j in enumerate(img_array):\n",
    "        for k,l in enumerate(j):\n",
    "            if l > 0:\n",
    "                x.append(i)\n",
    "                y.append(k)\n",
    "                \n",
    "    ## Create dataframe\n",
    "    img_df = pd.DataFrame()\n",
    "    img_df['x'] = x\n",
    "    img_df['y'] = y\n",
    "    \n",
    "    ## Initiate DBScan Model\n",
    "    img_dbscan = DBSCAN(eps, min_samples)\n",
    "    img_dbscan.fit(img_df)\n",
    "    \n",
    "    ## Get the labels (clusters)\n",
    "    img_df['labels'] = img_dbscan.labels_    \n",
    "    \n",
    "    ## Return the dataframe with labels\n",
    "    return img_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below function \"remove_clusters\" is to delete clusters which are good for further processing\n",
    "#### Following scenarios are considered to remove clusters\n",
    "\n",
    "- **Scenario 01:** Cluster with label -1. These data points does not have minimum samples to make a cluster\n",
    "- **Scenario 02:** Clusters with small length and width. Default is 5. Any cluster with just less than 5X5 are removed as they are very small clusters which are considered noise. Default threshold of length and width of the cluster can be change by the argument small_cluster_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:17.777901Z",
     "start_time": "2020-03-24T23:33:17.745813Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_clusters(img_df, small_cluster_threshold=5):\n",
    "    ## removing all data points which are not part of any cluster (-1)\n",
    "    img_df = img_df[img_df['labels'] > -1]\n",
    "       \n",
    "    ## Find min and max values for x & y co-ordinates for each cluster\n",
    "    cluster_min_points = img_df.groupby(['labels'],as_index=False).min()\n",
    "    cluster_max_points = img_df.groupby(['labels'],as_index=False).max()\n",
    "    \n",
    "    ## reset index of both above dataframes\n",
    "    cluster_min_points.reset_index(inplace=True)\n",
    "    cluster_max_points.reset_index(inplace=True)\n",
    "    \n",
    "    ## Get the labels\n",
    "    labels = img_df.labels\n",
    "    \n",
    "    # removing very small clusters which are not supposed to be there\n",
    "    # any cluster of size of less than small cluster threshold will be removed\n",
    "    for label in np.unique(labels):\n",
    "        if (cluster_max_points[cluster_max_points.labels==label]['x'].values[0] \\\n",
    "                  - cluster_min_points[cluster_min_points.labels==label]['x'].values[0] <= small_cluster_threshold) \\\n",
    "            or (cluster_max_points[cluster_max_points.labels==label]['y'].values[0] \\\n",
    "                  - cluster_min_points[cluster_min_points.labels==label]['y'].values[0] <= small_cluster_threshold):\n",
    "            img_df = img_df[img_df['labels'] != label]\n",
    "    \n",
    "    return img_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge_clusters function is used to merge or split clusters to correct character of the language\n",
    "\n",
    "##### This is the most complex function of all and in fact flawed in implementation. Needs revision in future to remove the bug of the code\n",
    "\n",
    "##### Following are the key steps of function\n",
    "- Clusters are merged where one cluster is completely overlaps another vertically. This is because in Telugu a character can split into multiple unconnected clusters like the character \"i\" in English.\n",
    "- In second scenario, clusters with overlap of 20 pixels are considered to merge into one. This is because in Telugu, it is not necessary that entire cluster to be overlapped with other cluster, for example 'క్య'\n",
    "- Merge processes is repeated at different thresholds starting from 20 then 15,10,5,0 and even -5 to improve the quality of the characters. Each stage the number resulted unique clusters are compared with number character labels available to the whole images if it fits are not. However, this step should be avoided as this can't be performed on complete new dataset.\n",
    "- Further clusters were split into small clusters when their length is too wider than average size. In this scenario as well the number of available character labels is used to determine average width of each character. Obviously this code should also be re-written to remove the bug of using target in improving quality of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:19.861142Z",
     "start_time": "2020-03-24T23:33:19.745693Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_clusters(img_df, char_labels):\n",
    "    \n",
    "    if img_df.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    ## Find min and max values for x & y co-ordinates for each cluster\n",
    "    cluster_min_points = img_df.groupby(['labels'],as_index=False).min()\n",
    "    cluster_max_points = img_df.groupby(['labels'],as_index=False).max()\n",
    "    \n",
    "    ## Get labels\n",
    "    labels = img_df.labels\n",
    "    unq_labels = np.unique(labels)\n",
    "\n",
    "    ## First merge based on full horizontal overlap\n",
    "    valid_label_list = [k for k in unq_labels]\n",
    "    for i in unq_labels:\n",
    "        for j in unq_labels:\n",
    "            if i == j or i not in valid_label_list or j not in valid_label_list:\n",
    "                continue\n",
    "            elif ((cluster_min_points[cluster_min_points.labels==i]['y'].values[0] \n",
    "                       - cluster_min_points[cluster_min_points.labels==j]['y'].values[0]) * \n",
    "                      (cluster_max_points[cluster_max_points.labels==i]['y'].values[0] \n",
    "                       - cluster_max_points[cluster_max_points.labels==j]['y'].values[0]) <= 0):\n",
    "                labels = [j if label == i else label for label in labels]\n",
    "                valid_label_list.remove(i)\n",
    "     \n",
    "    ## Partial overlaps\n",
    "    img_df.loc[:,'labels'] = labels\n",
    "    unq_labels = valid_label_list\n",
    "    ## Exit if the labels are equal to number of chars\n",
    "    if len(unq_labels) == len(char_labels):\n",
    "        #print('level01')\n",
    "        return img_df\n",
    "    elif len(unq_labels) > len(char_labels):\n",
    "        for threshold in range(20,-5,-11):\n",
    "            labels = img_df.labels \n",
    "            valid_label_list = [l for l in np.sort(np.unique(labels))]\n",
    "            unq_labels = valid_label_list\n",
    "            cluster_min_points = img_df.groupby(['labels'],as_index=False).min().reset_index(drop=True)\n",
    "            cluster_max_points = img_df.groupby(['labels'],as_index=False).max().reset_index(drop=True)\n",
    "\n",
    "            for i in unq_labels:\n",
    "                for j in unq_labels:                       \n",
    "                    if cluster_min_points[cluster_min_points.labels==i]['y'].values[0] \\\n",
    "                                        <= cluster_min_points[cluster_min_points.labels==j]['y'].values[0]:\n",
    "                        lc = i\n",
    "                        rc = j\n",
    "                    else:\n",
    "                        lc = j\n",
    "                        rc = i\n",
    "\n",
    "                    if (i != j) and (cluster_max_points[cluster_max_points.labels==lc]['y'].values[0] \\\n",
    "                          - cluster_min_points[cluster_min_points.labels==rc]['y'].values[0] >= threshold):\n",
    "                        labels = [rc if label == lc else label for label in labels]\n",
    "                        try:\n",
    "                            valid_label_list.remove(lc)\n",
    "                        except:\n",
    "                            None\n",
    "            if len(valid_label_list) == len(char_labels):\n",
    "                img_df.loc[:,'labels'] = labels\n",
    "                return img_df\n",
    "            elif len(valid_label_list) < len(char_labels):\n",
    "                img_df_t = img_df.copy()\n",
    "                img_df_t.loc[:,'labels'] = labels\n",
    "                max_pixel_y = np.max(img_df_t.y)\n",
    "                min_pixel_y = np.min(img_df_t.y)\n",
    "                char_avg_size = (max_pixel_y - min_pixel_y) / len(char_labels)\n",
    "                cluster_min_points = img_df_t.groupby(['labels'],as_index=False).min().reset_index(drop=True)\n",
    "                cluster_max_points = img_df_t.groupby(['labels'],as_index=False).max().reset_index(drop=True)\n",
    "                cluster_mm_points = cluster_min_points.merge(cluster_max_points,on='labels',suffixes=('_min', '_max'))\n",
    "                cluster_lengths = cluster_mm_points.y_max - cluster_mm_points.y_min\n",
    "                assumed_chars_in_each_cluster = np.round(cluster_lengths/char_avg_size,0)\n",
    "                result_char_length_after_split = cluster_lengths / assumed_chars_in_each_cluster \n",
    "                result_char_confidense_rate = np.round((result_char_length_after_split / char_avg_size)*100,0)\n",
    "                cluster_mm_points['assumed_chars_in_each_cluster'] = assumed_chars_in_each_cluster\n",
    "                cluster_mm_points['result_char_confidense_rate'] = result_char_confidense_rate\n",
    "                cluster_mm_points['accept_split'] = ['Yes' if cr in range(80,121) else 'No' \\\n",
    "                                                     for cr in result_char_confidense_rate]\n",
    "                cluster_mm_points['chars_final'] = cluster_mm_points.apply(lambda x: x[5] \\\n",
    "                                                                           if x[7] == 'Yes' else 1,axis=1)\n",
    "                \n",
    "                max_label = np.max(cluster_mm_points.labels) + 5\n",
    "                flag = 1\n",
    "                new_clusters = dict()\n",
    "                for i,j,k in cluster_mm_points.apply(lambda x:(x[2],x[4],x[8]), axis=1):\n",
    "                    if int(k) == 1:\n",
    "                        new_clusters[flag] = {'range_l':i,'range_u':j,'new_label':max_label + flag}\n",
    "                        flag += 1\n",
    "                    else:\n",
    "                        f = int((j - i) / k) \n",
    "                        for l in range(int(k)):\n",
    "                            new_clusters[flag] = {'range_l':i+f*l+1*(l>0*1), \\\n",
    "                                                  'range_u':i+f*(l+1),'new_label':max_label + flag}\n",
    "                            flag += 1\n",
    "\n",
    "                for i in range(len(new_clusters)):\n",
    "                    img_df_t.loc[img_df_t[(img_df_t.y >= new_clusters[i+1]['range_l']) \\\n",
    "                                   & (img_df_t.y <= new_clusters[i+1]['range_u'])].index,'labels'] \\\n",
    "                                                            = int(new_clusters[i+1]['new_label'])\n",
    "                if(len(np.unique(img_df_t.labels)) == len(char_labels)):\n",
    "                    #print('level03')\n",
    "                    return img_df_t\n",
    "    elif len(unq_labels) < len(char_labels):\n",
    "        labels = img_df.labels\n",
    "        valid_label_list = [l for l in np.sort(np.unique(labels))]\n",
    "        unq_labels = valid_label_list \n",
    "        cluster_min_points = img_df.groupby(['labels'],as_index=False).min().reset_index(drop=True)\n",
    "        cluster_max_points = img_df.groupby(['labels'],as_index=False).max().reset_index(drop=True)\n",
    "\n",
    "        for i in unq_labels:\n",
    "            for j in unq_labels:                       \n",
    "                if cluster_min_points[cluster_min_points.labels==i]['y'].values[0] \\\n",
    "                                    <= cluster_min_points[cluster_min_points.labels==j]['y'].values[0]:\n",
    "                    lc = i\n",
    "                    rc = j\n",
    "                else:\n",
    "                    lc = j\n",
    "                    rc = i\n",
    "\n",
    "                if (i != j) and (cluster_max_points[cluster_max_points.labels==lc]['y'].values[0] \\\n",
    "                      - cluster_min_points[cluster_min_points.labels==rc]['y'].values[0] >= 20):\n",
    "                    labels = [rc if label == lc else label for label in labels]\n",
    "                    try:\n",
    "                        valid_label_list.remove(lc)\n",
    "                    except:\n",
    "                        None         \n",
    "        ## Splitting immedietly after merging.\n",
    "        for confidence_threshold in range(5,21,5):\n",
    "            img_df_t = img_df.copy()\n",
    "            img_df_t.reset_index(drop=True)\n",
    "            max_pixel_y = np.max(img_df_t.y)\n",
    "            min_pixel_y = np.min(img_df_t.y)\n",
    "            char_avg_size = (max_pixel_y - min_pixel_y) / len(char_labels)\n",
    "            cluster_min_points = img_df_t.groupby(['labels'],as_index=False).min().reset_index(drop=True)\n",
    "            cluster_max_points = img_df_t.groupby(['labels'],as_index=False).max().reset_index(drop=True)\n",
    "            cluster_mm_points = cluster_min_points.merge(cluster_max_points,on='labels',suffixes=('_min', '_max'))\n",
    "            cluster_lengths = cluster_mm_points.y_max - cluster_mm_points.y_min\n",
    "            assumed_chars_in_each_cluster = np.round(cluster_lengths/char_avg_size,0)\n",
    "            result_char_length_after_split = cluster_lengths / assumed_chars_in_each_cluster \n",
    "            result_char_confidense_rate = np.round((result_char_length_after_split / char_avg_size)*100,0)\n",
    "            cluster_mm_points['assumed_chars_in_each_cluster'] = assumed_chars_in_each_cluster\n",
    "            cluster_mm_points['result_char_confidense_rate'] = result_char_confidense_rate\n",
    "            cluster_mm_points['accept_split'] = ['Yes' if cr in range(100-confidence_threshold,100+\\\n",
    "                                                                      confidence_threshold+1) \\\n",
    "                                                 else 'No' for cr in result_char_confidense_rate]\n",
    "            cluster_mm_points['chars_final'] = cluster_mm_points.apply(lambda x: x[5] if x[7] == 'Yes' else 1,axis=1)\n",
    "\n",
    "            max_label = np.max(cluster_mm_points.labels) + 5\n",
    "            flag = 1\n",
    "            new_clusters = dict()\n",
    "            for i,j,k in cluster_mm_points.apply(lambda x:(x[2],x[4],x[8]), axis=1):\n",
    "                if int(k) == 1:\n",
    "                    new_clusters[flag] = {'range_l':i,'range_u':j,'new_label':max_label + flag}\n",
    "                    flag += 1\n",
    "                else:\n",
    "                    f = int((j - i) / k) \n",
    "                    for l in range(int(k)):\n",
    "                        new_clusters[flag] = {'range_l':i+f*l+1*(l>0*1),'range_u':i+f*(l+1),\\\n",
    "                                              'new_label':max_label + flag}\n",
    "                        if l == k-1:\n",
    "                            new_clusters[flag]['range_u'] = j\n",
    "                        flag += 1\n",
    "\n",
    "            for i in range(len(new_clusters)):\n",
    "                for i in range(len(new_clusters)):\n",
    "                    img_df_t.loc[img_df[(img_df_t.y >= new_clusters[i+1]['range_l']) \\\n",
    "                                       & (img_df_t.y <= new_clusters[i+1]['range_u'])].index,'labels'] \\\n",
    "                                                                = int(new_clusters[i+1]['new_label'])\n",
    "            if(len(np.unique(img_df_t.labels)) == len(char_labels)):\n",
    "                #print('level04')\n",
    "                return img_df_t\n",
    "    #print('level0F')\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort_clusters function sorts the character cluster in the order written in the image\n",
    "\n",
    "- This function uses the start position of each cluster on X-axis to sort the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:21.561063Z",
     "start_time": "2020-03-24T23:33:21.529277Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_clusters(img_df):\n",
    "    \n",
    "    if img_df.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "    labels = img_df.labels\n",
    "    max_lable = img_df['labels'].max() + 3\n",
    "    labels = [l + max_lable for l in labels]\n",
    "    img_df['labels'] = labels\n",
    "    lables_list = img_df.groupby(['labels'],as_index=False).min().sort_values(by='y')['labels'].values\n",
    "    \n",
    "    new_lable = 0\n",
    "    for lable in lables_list:\n",
    "        labels = [new_lable if l == lable else l for l in labels]\n",
    "        new_lable += 1\n",
    "    \n",
    "    ##update the labels after re-arrange process\n",
    "    img_df['labels'] = labels\n",
    "    \n",
    "    ## return the dataframe with final clusters\n",
    "    return img_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To keep the length and width of each character consistent the character clusters are standardized\n",
    "\n",
    "- Default length and width of the resultant character block is 100X100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:23.077964Z",
     "start_time": "2020-03-24T23:33:23.062334Z"
    }
   },
   "outputs": [],
   "source": [
    "def stadardize_clusters(img_char_clusters):\n",
    "    if img_char_clusters.shape[0] == 0:\n",
    "        return np.zeros(1)\n",
    "    \n",
    "    ## There assumption that the columns of the dataframe is x, y, labels\n",
    "    labels = img_char_clusters.labels\n",
    "    \n",
    "    Std_Clusters = np.zeros([len(np.unique(labels)),101,101])\n",
    "    char_pos = 0\n",
    "    \n",
    "    for label in np.sort(np.unique(labels)):\n",
    "        cluster = img_char_clusters[img_char_clusters.labels==label][['x','y']].copy()\n",
    "        cluster.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        MM_Scaler = MinMaxScaler((1,100))\n",
    "        Transformed_Cluster = np.round(MM_Scaler.fit_transform(cluster),0)\n",
    "        \n",
    "        Std_Cluster_Array = np.zeros([101,101])\n",
    "    \n",
    "        for i,j in (zip(Transformed_Cluster[:,0],Transformed_Cluster[:,1])):\n",
    "            Std_Cluster_Array[int(i),int(j)] = 1\n",
    "        \n",
    "        Std_Clusters[char_pos] =  Std_Cluster_Array\n",
    "        char_pos += 1\n",
    "    \n",
    "    return Std_Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the label words into label characters.\n",
    "\n",
    "- All the characters of the Telugu language are split into different groups\n",
    "- Vowels and Consonants can exist on their own.\n",
    "- If Vowel extensions are followed by a consonant, both the characters are combined to make a new character in UNICODE set\n",
    "- Vowel extensions can only followed by a consonant, while the characters in the group extensions_o can be preceded by both vowels and consonants\n",
    "- half_extension character is very key one which is used widely that takes two characters on either side to make a brand new character\n",
    "- Other half characters are also same as vowel extensions though less used in normal language.\n",
    "- The characters in the group other_full_chars are similar to consonants though these rare usage characters\n",
    "- Finally the characters in the group numbers are just the numbers 0-9 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:33:24.561450Z",
     "start_time": "2020-03-24T23:33:24.529162Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_char_labels(word_label):\n",
    "    vowels = ['అ','ఆ','ఇ','ఈ','ఉ','ఊ','ఋ','ౠ','ఌ','ౡ','ఎ','ఏ','ఐ','ఒ','ఓ','ఔ']\n",
    "    consonants = ['క','ఖ','గ','ఘ','ఙ','చ','ఛ','జ','ఝ','ఞ',\n",
    "                  'ట','ఠ','డ','ఢ','ణ','త','థ','ద','ధ','న',\n",
    "                  'ప','ఫ','బ','భ','మ','య','ర','ల','వ','స','ష','శ','హ','ళ','ఱ']\n",
    "    vowel_extensions = ['ా','ి','ీ','ు','ూ','ృ','ౄ','ె','ే','ై','ొ','ో','ౌ']\n",
    "    extentions_o = ['ం','ః']\n",
    "    half_extentions = ['్']\n",
    "    other_half_chars = ['ఁ','ౢ','ౣ','ౖ','ఀ']\n",
    "    other_full_chars = ['ౘ','ౙ','ఽ','ఴ','౸','౹','౺','౻','౼','౽','౾','౿']\n",
    "    numbers = ['౦','౧','౨','౩','౪','౫','౬','౭','౮','౯']\n",
    "    \n",
    "    char_labels = []\n",
    "    half_char_flag = 0\n",
    "    \n",
    "    for c in word_label:\n",
    "        if c in vowels or c in extentions_o:\n",
    "            char_labels.append(c)\n",
    "        elif c in consonants:\n",
    "            if half_char_flag == 1:\n",
    "                char = char_labels[-1] + c\n",
    "                del char_labels[-1]\n",
    "                char_labels.append(char)\n",
    "                half_char_flag = 0\n",
    "            else:\n",
    "                char_labels.append(c)\n",
    "        elif c in vowel_extensions or c in other_half_chars:\n",
    "            char = char_labels[-1] + c\n",
    "            del char_labels[-1]\n",
    "            char_labels.append(char)\n",
    "        elif c in half_extentions:\n",
    "            char = char_labels[-1] + c\n",
    "            del char_labels[-1]\n",
    "            char_labels.append(char)\n",
    "            half_char_flag = 1\n",
    "        elif c == '-' or c in other_full_chars or c in numbers:\n",
    "            char_labels.append(c)\n",
    "    \n",
    "    return char_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T23:46:03.578938Z",
     "start_time": "2020-03-24T23:45:58.169083Z"
    }
   },
   "outputs": [],
   "source": [
    "## Source file which has all the names of the image dataset and the word labels\n",
    "train_file_name = 'train.txt'\n",
    "train_data = pd.read_csv(train_file_name,sep=' ',header=None)\n",
    "train_data.columns=['image','label']\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "## This list is used later to analyze the issues faced with files so that they can be analyzed\n",
    "issue_files = []\n",
    "\n",
    "cntr = 1\n",
    "for i in range(train_data.shape[0]):\n",
    "    ## Get the image object\n",
    "    img = Image.open(train_data.image[i])\n",
    "    cleaned_img = clean_image(img,10)\n",
    "    if np.array(cleaned_img).sum() == 0:\n",
    "        issue_files.append(train_data.image[i] + ',' + train_data.label[i] + ',EmptyFile')\n",
    "        continue\n",
    "    ## Following if the code perform different steps of the EDA\n",
    "    char_labels = get_char_labels(train_data.label[i])    \n",
    "    char_clusters_v1 = find_clusters(cleaned_img,eps=2,min_samples=3)\n",
    "    char_clusters_v2 = remove_clusters(char_clusters_v1, small_cluster_threshold=10)\n",
    "    char_clusters_v3 = merge_clusters(char_clusters_v2, char_labels)\n",
    "    char_clusters_v4 = sort_clusters(char_clusters_v3)\n",
    "    std_char_clusters = stadardize_clusters(char_clusters_v4)\n",
    "    \n",
    "    ## Check if the process is successful to get correct number of character cluster along with their labels\n",
    "    if char_clusters_v3.shape[0] > 0:\n",
    "        for r in range(len(char_labels)):\n",
    "            row = [train_data.image[i],r] + std_char_clusters[r].reshape(1,10201).tolist()[0] + [char_labels[r]]\n",
    "            row_df = pd.DataFrame([row])\n",
    "            data_df = pd.concat([data_df, row_df], axis=0)\n",
    "    else:\n",
    "        issue_files.append(train_data.image[i]+','+train_data.label[i])\n",
    "        \n",
    "    if data_df.shape[0]/500 > 1:\n",
    "        file_name = 'data_df_' + str(cntr) + '.csv'\n",
    "        data_df.to_csv(file_name,index=False)\n",
    "        data_df = pd.DataFrame()\n",
    "        cntr += 1\n",
    "        \n",
    "file_name = 'data_df_' + str(cntr) + '.csv'\n",
    "data_df.to_csv(file_name,index=False)\n",
    "\n",
    "pd.DataFrame({'Issue_File,Issue_Label':issue_files}).to_csv('Issues_Files.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
